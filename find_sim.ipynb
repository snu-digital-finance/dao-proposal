{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0x56c145509b9b5d613236ffd6810c037130b20878243e99728772716a89dc70f6\n",
    "- Business: User Expansion Strategy -> False: Info/Announcement\n",
    "\n",
    "\n",
    "$cake price\n",
    "- token reward adjustment\n",
    "- ???? https://snapshot.org/#/cakevote.eth/proposal/0x8d22df8165ffda034dba6a092ee9743e769ca466813b4b162e3646a62e4a1c10\n",
    "\n",
    "\n",
    "0xb0b9c229eab715c2752b3cdb652600d1e4fdef21d227de8e037ab9f50e290be5\n",
    "- Business Fee? trading liquidity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "crypto wiki 긁어서 해당 단어의 설명글을 바탕으로 그 단어의 어조 파악?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20750, 39), (264, 39))"
      ]
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class_cols = [\"Class A\", \"Class B\", \"Class C\"]\n",
    "\n",
    "df = pd.read_excel(\"proposals_preprocess_0812_cleaned.xlsx\")\n",
    "df = df.replace('', None)\n",
    "df.shape, df[~df[class_cols].isna().all(axis=1)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1195, 39)"
      ]
     },
     "execution_count": 459,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = pd.read_excel(\"proposals_preprocess_0812_cleaned_cached.xlsx\")\n",
    "temp = temp.replace('', None)\n",
    "temp = temp[~temp[class_cols].isna().all(axis=1)]\n",
    "temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20750, 39), (1196, 39))"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for d in [df, temp]:\n",
    "    d[\"Class A\"] = d[\"Class A\"].str.strip()\n",
    "    d[\"Class B\"] = d[\"Class B\"].str.strip()\n",
    "    d[\"Class C\"] = d[\"Class C\"].str.strip().str.lower()\n",
    "\n",
    "    for k, v in {\n",
    "        \"User expanshion Strategy\": \"User Expansion Strategy\",\n",
    "        \"User expansion Strategy\": \"User Expansion Strategy\",\n",
    "        \"User expansion\": \"User Expansion Strategy\",\n",
    "        \"token reward adjustment\": \"Token Reward Adjustment\",\n",
    "        \"token allocation\": \"Token Allocation\",\n",
    "        \"Add new token\": \"Expansion\",\n",
    "        \"Reward adjustment\": \"Reward Adjustment\",\n",
    "        \"Liquidity management\": \"Liquidity Management\",\n",
    "        \"inclusive\": \"Inclusive\",\n",
    "        \"restrictive\": \"Restrictive\",\n",
    "        \"-\": None\n",
    "    }.items():\n",
    "        d.loc[d[\"Class B\"] == k, \"Class B\"] = v\n",
    "\n",
    "for row in temp.iterrows():\n",
    "    for c in class_cols:\n",
    "        df.loc[df[\"id\"] == row[1].id, c] = row[1][c]\n",
    "\n",
    "df.shape, df[~df[class_cols].isna().all(axis=1)].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Class A           Class B                \n",
       "-                 NaN                            1\n",
       "Business          Expansion                     41\n",
       "                  Fee                           59\n",
       "                  New Business Proposal          5\n",
       "                  Rate                           4\n",
       "                  Reward Adjustment             33\n",
       "                  User Expansion Strategy       12\n",
       "F1                request money                269\n",
       "F2                no detailed content            6\n",
       "F3                Off-topic                     37\n",
       "F4                Info/Announcement             64\n",
       "Governance        Inclusive                     21\n",
       "                  Restrictive                    1\n",
       "                  Security                       1\n",
       "                  adjust                         1\n",
       "                  feature improvement            5\n",
       "                  NaN                            5\n",
       "Opearations       Team                           2\n",
       "Operations        Feature Improvement            1\n",
       "                  Liquidity Management          41\n",
       "                  Risk Management               18\n",
       "                  Security                      20\n",
       "                  Team                         313\n",
       "                  User friendly features       132\n",
       "                  feature improvement           12\n",
       "                  NaN                           14\n",
       "Reserve/Treasury  Asset Management               3\n",
       "                  Donation                       6\n",
       "                  Grant                          3\n",
       "                  Reduce                         1\n",
       "                  Requesting Grant               3\n",
       "                  Steward                        1\n",
       "                  Token Sale                     1\n",
       "                  NaN                            4\n",
       "tokenomics        Adjust Supply                  7\n",
       "                  Token Allocation               1\n",
       "                  Token Reward Adjustment       26\n",
       "                  grant for incentive            4\n",
       "                  NaN                            2\n",
       "NaN               O                             14\n",
       "                  NaN                        19556\n",
       "dtype: int64"
      ]
     },
     "execution_count": 461,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[[\"Class A\", \"Class B\"]].value_counts(dropna=False).sort_index() #.to_csv(\"class_counts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = df.groupby([\"Class A\", \"Class B\", \"Class C\"], dropna=False)\n",
    "temp = []\n",
    "for name, group in groups:\n",
    "    temp.append(group.head(10))\n",
    "temp = pd.concat(temp)\n",
    "temp[[\"id\", \"link\", \"title\", \"body\", \"author\", \"votes\", \"choices\", \"space_id\", \"Class A\", \"Class B\", \"Class C\"]].to_excel(\"sample_category.xlsx\", engine=\"openpyxl\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Cleansing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag, ne_chunk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "nltk.download('maxent_ne_chunker', quiet=True)\n",
    "nltk.download('words', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remain only lower case english words\n",
    "for v in [\"title\", \"body\"]:\n",
    "    df[f\"{v}_nlp\"] = df[f\"{v}_lower\"].astype(str) \\\n",
    "        .str.replace(\"[^a-z\\n]\", \" \", regex=True).str.strip() \\\n",
    "            .apply(lambda x: ' '.join(\n",
    "            (w for w in word_tokenize(x) if not w in stop_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -q --user nltk pattern xlsxwriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "from pattern.en import lemma, singularize\n",
    "from nltk.corpus import wordnet\n",
    "import numpy as np\n",
    "\n",
    "skip_words = lambda arr: np.vectorize(lambda x: ' '.join(\n",
    "        xx for xx in x.split() if xx not in arr\n",
    "    ).strip())\n",
    "    \n",
    "is_word_in_dictionary = lambda word: any(\n",
    "    w for w in set([word, singularize(word), lemma(word)]) \n",
    "    if len(wordnet.synsets(w)) > 0\n",
    ")\n",
    "    \n",
    "print(is_word_in_dictionary(\"writing\"))  # True (-ing로 끝나는 동사가 기본형으로 변환됨)\n",
    "print(is_word_in_dictionary(\"apples\"))  # True (복수형 단어가 기본형으로 변환됨)\n",
    "print(is_word_in_dictionary(\"ate\"))     # True (과거형으로 변환된 단어가 기본형으로 변환됨)\n",
    "print(is_word_in_dictionary(\"qwerty\"))  # False (사전에 없는 단어)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove protocol names and coin symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(245,), (264,), (11429,), (11429,), (14354,), (7,)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(34572,)"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import requests\n",
    "\n",
    "words = []\n",
    "\n",
    "# dao symbol\n",
    "func = np.vectorize(lambda x: re.sub(r\"\\.eth\", \"\", x).strip())\n",
    "words.append(\n",
    "    func(df[\"space_id\"].unique())\n",
    ")\n",
    "\n",
    "# dao symbol except \"gov\", \"dao\", \"vote\"\n",
    "func = np.vectorize(lambda x: re.sub(\"\\.|\\-\", \" \", re.sub(r\"\\.eth|gov|dao|vote\", \"\", x)).strip())\n",
    "words.append(\n",
    "    np.sort(np.array([word for sentence in func(df[\"space_id\"].unique()) for word in sentence.split() if len(word) > 1]))\n",
    ")\n",
    "\n",
    "# All of coin symbol\n",
    "response = requests.get(\"https://api.coingecko.com/api/v3/coins/list\")\n",
    "words.append(\n",
    "    np.array([x for x in set(coin['symbol'].lower() for coin in response.json())])\n",
    ")\n",
    "words.append(\n",
    "    np.array([x for x in set(coin['symbol'].lower()+\"s\" for coin in response.json())])\n",
    ")\n",
    "words.append(\n",
    "    np.array([x for x in set(coin['name'].lower() for coin in response.json())])\n",
    ")\n",
    "\n",
    "# Communication Channel Name\n",
    "words.append(\n",
    "    np.array([\"tiktok\", \"linkedin\", \"github\", \"gitbook\", \"facebook\", \"reddit\", \"discord\"])\n",
    "    # medium...\n",
    ")\n",
    "\n",
    "print([v.shape for v in words])\n",
    "words = np.unique(np.concatenate(words))\n",
    "words.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "func = skip_words(words)\n",
    "df[\"body_nlp\"] = func(df[\"body_nlp\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50670\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "word_counts = Counter((\n",
    "    item \n",
    "    for sub in (\n",
    "        str(x).split() for x in df[\"body_nlp\"]\n",
    "    ) \n",
    "    for item in sub\n",
    "))\n",
    "print(len(word_counts))\n",
    "\n",
    "\n",
    "words = [x for x in word_counts.keys() \n",
    "    if ((\"dao\" in x and len(x) > 4) or (\"swap\" in x and len(x) > 5) or \"frax\" in x)\n",
    "    and not is_word_in_dictionary(x)]\n",
    "print(len(words))\n",
    "\n",
    "func = skip_words(words)\n",
    "df[\"body_nlp\"] = func(df[\"body_nlp\"].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove rare words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26674\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'crscia',\n",
       " 'dbochman',\n",
       " 'fwgs',\n",
       " 'nueral',\n",
       " 'emilianos',\n",
       " 'yveth',\n",
       " 'basile',\n",
       " 'landshares',\n",
       " 'rockaway',\n",
       " 'zal',\n",
       " 'memeability',\n",
       " 'mimicfi',\n",
       " 'qimp',\n",
       " 'rpsbet',\n",
       " 'mikr',\n",
       " 'rewardgauge',\n",
       " 'govuk',\n",
       " 'nbn',\n",
       " 'txbrowser',\n",
       " 'crowdsourced',\n",
       " 'microgravity',\n",
       " 'problemas',\n",
       " 'anymagik',\n",
       " 'microservices',\n",
       " 'yng',\n",
       " 'blockdaemon',\n",
       " 'cornelius',\n",
       " 'newscience',\n",
       " 'higherhigh',\n",
       " 'crypo',\n",
       " 'angelvault',\n",
       " 'extempt',\n",
       " 'berea',\n",
       " 'cryptocomic',\n",
       " 'gascostbean',\n",
       " 'terralabs',\n",
       " 'kasa',\n",
       " 'escan',\n",
       " 'consideation',\n",
       " 'siriwat',\n",
       " 'foveated',\n",
       " 'qmcvmeu',\n",
       " 'hu',\n",
       " 'tickbitmap',\n",
       " 'minigames',\n",
       " 'parallelizationand',\n",
       " 'beaconcha',\n",
       " 'islandid',\n",
       " 'kycs',\n",
       " 'usership',\n",
       " 'sophie',\n",
       " 'jewfuckerr',\n",
       " 'bamm',\n",
       " 'exomads',\n",
       " 'rerouting',\n",
       " 'npos',\n",
       " 'publius',\n",
       " 'listingmanager',\n",
       " 'sveltekit',\n",
       " 'vestingvault',\n",
       " 'triaged',\n",
       " 'bicocca',\n",
       " 'upsells',\n",
       " 'vuhusx',\n",
       " 'wetrust',\n",
       " 'imageuploader',\n",
       " 'pcsv',\n",
       " 'congruentlabs',\n",
       " 'gset',\n",
       " 'roshangar',\n",
       " 'mirrorx',\n",
       " 'jhan',\n",
       " 'shopee',\n",
       " 'mahoro',\n",
       " 'conjunto',\n",
       " 'nene',\n",
       " 'teamup',\n",
       " 'meatspace',\n",
       " 'tobas',\n",
       " 'suprise',\n",
       " 'dougie',\n",
       " 'prelaunch',\n",
       " 'gmo',\n",
       " 'goalscoring',\n",
       " 'rashomon',\n",
       " 'caplan',\n",
       " 'shira',\n",
       " 'commnunity',\n",
       " 'nuraghi',\n",
       " 'jackieboi',\n",
       " 'occultoccurrences',\n",
       " 'marketwide',\n",
       " 'nadim',\n",
       " 'anybody',\n",
       " 'digifizzy',\n",
       " 'nonbinding',\n",
       " 'lefteris',\n",
       " 'cartan',\n",
       " 'auryn',\n",
       " 'gabbana',\n",
       " 'gamifying',\n",
       " 'moonbirs',\n",
       " 'lauc',\n",
       " 'liquely',\n",
       " 'cmd',\n",
       " 'optina',\n",
       " 'especificas',\n",
       " 'unripefacet',\n",
       " 'addliquidity',\n",
       " 'legionnary',\n",
       " 'mobiustripper',\n",
       " 'tintedswan',\n",
       " 'erease',\n",
       " 'discloser',\n",
       " 'pixelvaults',\n",
       " 'radicalxchange',\n",
       " 'fron',\n",
       " 'interrested',\n",
       " 'masterchefs',\n",
       " 'anount',\n",
       " 'simor',\n",
       " 'karpartkey',\n",
       " 'foulk',\n",
       " 'hmsr',\n",
       " 'linkedwearables',\n",
       " 'pcrv',\n",
       " 'annemarie',\n",
       " 'victorstark',\n",
       " 'membershipas',\n",
       " 'stoney',\n",
       " 'rickyday',\n",
       " 'kalmar',\n",
       " 'tekken',\n",
       " 'vechs',\n",
       " 'nfted',\n",
       " 'raydiums',\n",
       " 'streetcred',\n",
       " 'martino',\n",
       " 'hootsuite',\n",
       " 'proliferator',\n",
       " 'jasonz',\n",
       " 'governoralpha',\n",
       " 'beale',\n",
       " 'surtr',\n",
       " 'composablestablepoolv',\n",
       " 'meantwhat',\n",
       " 'panjshir',\n",
       " 'biennale',\n",
       " 'bornless',\n",
       " 'allcoredevs',\n",
       " 'carebear',\n",
       " 'mamoru',\n",
       " 'jumptask',\n",
       " 'disadvise',\n",
       " 'givfi',\n",
       " 'cvgfinance',\n",
       " 'arpril',\n",
       " 'heremes',\n",
       " 'poy',\n",
       " 'playership',\n",
       " 'maxblidperusd',\n",
       " 'sushihouse',\n",
       " 'fydes',\n",
       " 'ferdinando',\n",
       " 'hanson',\n",
       " 'balanceofpenalizedunderlying',\n",
       " 'dreamteam',\n",
       " 'cronje',\n",
       " 'vett',\n",
       " 'develope',\n",
       " 'brlusd',\n",
       " 'contadas',\n",
       " 'blockscape',\n",
       " 'preloaded',\n",
       " 'curvecryptolppricefeed',\n",
       " 'spcujed',\n",
       " 'fanadise',\n",
       " 'mudge',\n",
       " 'xirtam',\n",
       " 'incentivised',\n",
       " 'artistcert',\n",
       " 'tenz',\n",
       " 'striatal',\n",
       " 'boredapes',\n",
       " 'fermin',\n",
       " 'lxpoints',\n",
       " 'safiri',\n",
       " 'filmer',\n",
       " 'cvxalusd',\n",
       " 'proxyadmin',\n",
       " 'jjwc',\n",
       " 'ninguno',\n",
       " 'idc',\n",
       " 'miske',\n",
       " 'cryptocakir',\n",
       " 'freecity',\n",
       " 'catu',\n",
       " 'keychain',\n",
       " 'hodlrs',\n",
       " 'minteo',\n",
       " 'athelete',\n",
       " 'menezes',\n",
       " 'misfolded',\n",
       " 'abz',\n",
       " 'perdigones',\n",
       " 'rocketx',\n",
       " 'incretins',\n",
       " 'priyanka',\n",
       " 'namewrapper',\n",
       " 'tayasa',\n",
       " 'kemper',\n",
       " 'rhebb',\n",
       " 'sublight',\n",
       " 'chanin',\n",
       " 'getters',\n",
       " 'vauge',\n",
       " 'sendrawtransactionconditional',\n",
       " 'lrtflywheel',\n",
       " 'mimaklas',\n",
       " 'poignard',\n",
       " 'babykongs',\n",
       " 'vxm',\n",
       " 'vilte',\n",
       " 'puskas',\n",
       " 'bratz',\n",
       " 'vewom',\n",
       " 'shifu',\n",
       " 'rybak',\n",
       " 'immortaltreearms',\n",
       " 'llegar',\n",
       " 'juxton',\n",
       " 'downvoted',\n",
       " 'verificados',\n",
       " 'dropdowns',\n",
       " 'godfrey',\n",
       " 'ifm',\n",
       " 'xbnt',\n",
       " 'bhuta',\n",
       " 'mainchain',\n",
       " 'unicorndroppingz',\n",
       " 'vimeo',\n",
       " 'yugalab',\n",
       " 'isgameopen',\n",
       " 'distrubtuion',\n",
       " 'rugendyke',\n",
       " 'postgresql',\n",
       " 'airtime',\n",
       " 'yxue',\n",
       " 'noema',\n",
       " 'incentivesthis',\n",
       " 'chowies',\n",
       " 'phemex',\n",
       " 'gmlink',\n",
       " 'speraxs',\n",
       " 'expectedly',\n",
       " 'relaunched',\n",
       " 'stablepoolfactory',\n",
       " 'grinstein',\n",
       " 'karao',\n",
       " 'ridgewood',\n",
       " 'kaxline',\n",
       " 'infuencers',\n",
       " 'metatx',\n",
       " 'avilches',\n",
       " 'ammendments',\n",
       " 'posibilities',\n",
       " 'alasset',\n",
       " 'obius',\n",
       " 'goalsimportant',\n",
       " 'clich',\n",
       " 'alexandrov',\n",
       " 'maaroufi',\n",
       " 'dmat',\n",
       " 'flutterflow',\n",
       " 'akhen',\n",
       " 'artdrops',\n",
       " 'pzeip',\n",
       " 'canidio',\n",
       " 'pancakedwap',\n",
       " 'swxt',\n",
       " 'calchulus',\n",
       " 'unlinkable',\n",
       " 'disaggregate',\n",
       " 'hpxl',\n",
       " 'vefpis',\n",
       " 'ideassaving',\n",
       " 'hyperspectral',\n",
       " 'basicaly',\n",
       " 'mspell',\n",
       " 'nftp',\n",
       " 'compositable',\n",
       " 'idletokens',\n",
       " 'dfp',\n",
       " 'ssid',\n",
       " 'suckaburger',\n",
       " 'prebys',\n",
       " 'calsibot',\n",
       " 'qna',\n",
       " 'creando',\n",
       " 'rwaste',\n",
       " 'xeko',\n",
       " 'retrayable',\n",
       " 'scrollbridgeexecuror',\n",
       " 'ein',\n",
       " 'beanfts',\n",
       " 'cg',\n",
       " 'dclmvmf',\n",
       " 'srijith',\n",
       " 'rescuetokens',\n",
       " 'bananachain',\n",
       " 'kemp',\n",
       " 'usdcoin',\n",
       " 'gdigg',\n",
       " 'curations',\n",
       " 'maxdebt',\n",
       " 'willa',\n",
       " 'maraoz',\n",
       " 'voss',\n",
       " 'percentagewhether',\n",
       " 'tokenflow',\n",
       " 'molins',\n",
       " 'tovah',\n",
       " 'ksx',\n",
       " 'cumplir',\n",
       " 'setminter',\n",
       " 'deathmatch',\n",
       " 'nouncils',\n",
       " 'bain',\n",
       " 'stableusd',\n",
       " 'slagathorthemammothking',\n",
       " 'tocwg',\n",
       " 'peralta',\n",
       " 'netpac',\n",
       " 'optimiums',\n",
       " 'elonvitalik',\n",
       " 'growshop',\n",
       " 'pancakesquad',\n",
       " 'eak',\n",
       " 'ethtlv',\n",
       " 'newpolicyipfsuri',\n",
       " 'dumpoor',\n",
       " 'ultramono',\n",
       " 'gmoshiro',\n",
       " 'shootermcgavin',\n",
       " 'viem',\n",
       " 'costumisable',\n",
       " 'enjins',\n",
       " 'pxbtrfly',\n",
       " 'demonsate',\n",
       " 'steamfest',\n",
       " 'hadid',\n",
       " 'favortoken',\n",
       " 'unaffordable',\n",
       " 'pedersen',\n",
       " 'aarve',\n",
       " 'buena',\n",
       " 'unitranche',\n",
       " 'perianne',\n",
       " 'jckbutcher',\n",
       " 'coinflip',\n",
       " 'expries',\n",
       " 'adserver',\n",
       " 'underpowered',\n",
       " 'schaedler',\n",
       " 'xangle',\n",
       " 'needcan',\n",
       " 'burrus',\n",
       " 'killam',\n",
       " 'spiritv',\n",
       " 'doots',\n",
       " 'underfunding',\n",
       " 'okus',\n",
       " 'expertbandit',\n",
       " 'hexens',\n",
       " 'nofuture',\n",
       " 'bddd',\n",
       " 'fileters',\n",
       " 'anytrust',\n",
       " 'cutscheck',\n",
       " 'cryptoavatars',\n",
       " 'ef',\n",
       " 'wethv',\n",
       " 'blit',\n",
       " 'seevi',\n",
       " 'ywvacwwveb',\n",
       " 'clonescody',\n",
       " 'ttulo',\n",
       " 'crvplain',\n",
       " 'bl',\n",
       " 'roustan',\n",
       " 'onramper',\n",
       " 'jamarillo',\n",
       " 'unburned',\n",
       " 'quantuim',\n",
       " 'menoage',\n",
       " 'baes',\n",
       " 'autocompouding',\n",
       " 'abhishek',\n",
       " 'fbl',\n",
       " 'sirsus',\n",
       " 'ethrome',\n",
       " 'whitelisters',\n",
       " 'aydin',\n",
       " 'nouner',\n",
       " 'jangos',\n",
       " 'niteri',\n",
       " 'smoe',\n",
       " 'geest',\n",
       " 'boto',\n",
       " 'vliz',\n",
       " 'johnnysharp',\n",
       " 'fractilians',\n",
       " 'benvan',\n",
       " 'chong',\n",
       " 'nubarron',\n",
       " 'befor',\n",
       " 'sbcs',\n",
       " 'anwar',\n",
       " 'muggleplay',\n",
       " 'minhealthfactor',\n",
       " 'pking',\n",
       " 'monti',\n",
       " 'yararasita',\n",
       " 'wraynman',\n",
       " 'nervetrip',\n",
       " 'imagefromurl',\n",
       " 'veltx',\n",
       " 'stashrewarddistro',\n",
       " 'alessio',\n",
       " 'creaete',\n",
       " 'beamit',\n",
       " 'buhangin',\n",
       " 'crowdsources',\n",
       " 'xjx',\n",
       " 'removalbasedeposit',\n",
       " 'miguel',\n",
       " 'shardyaco',\n",
       " 'metrika',\n",
       " 'hreth',\n",
       " 'codedrop',\n",
       " 'dtokens',\n",
       " 'xz',\n",
       " 'leith',\n",
       " 'oppurtunities',\n",
       " 'ipts',\n",
       " 'crowdedness',\n",
       " 'rl',\n",
       " 'lewwt',\n",
       " 'kattanas',\n",
       " 'pomptu',\n",
       " 'lesaege',\n",
       " 'avaialble',\n",
       " 'syntherium',\n",
       " 'lorez',\n",
       " 'launchub',\n",
       " 'contente',\n",
       " 'choonaikos',\n",
       " 'opportunites',\n",
       " 'nfti',\n",
       " 'aglink',\n",
       " 'sfarx',\n",
       " 'tokentable',\n",
       " 'minigolf',\n",
       " 'tokenbound',\n",
       " 'figma',\n",
       " 'vict',\n",
       " 'olasunkanmi',\n",
       " 'finra',\n",
       " 'osx',\n",
       " 'ingalandia',\n",
       " 'eeuroc',\n",
       " 'silenciar',\n",
       " 'coloredg',\n",
       " 'sourecred',\n",
       " 'basefee',\n",
       " 'brandan',\n",
       " 'oracleless',\n",
       " 'beetsbar',\n",
       " 'beraerts',\n",
       " 'flowershop',\n",
       " 'takeabreath',\n",
       " 'capsplusrisksteward',\n",
       " 'firsanov',\n",
       " 'hyperplay',\n",
       " 'swip',\n",
       " 'vistors',\n",
       " 'onand',\n",
       " 'questionaires',\n",
       " 'kpxks',\n",
       " 'infosec',\n",
       " 'sendtoken',\n",
       " 'btcchinabtcchinabtcchinabtckr',\n",
       " 'ckmutpn',\n",
       " 'gldo',\n",
       " 'dikpick',\n",
       " 'impl',\n",
       " 'elegirn',\n",
       " 'thay',\n",
       " 'auctionhouse',\n",
       " 'maryana',\n",
       " 'finnaly',\n",
       " 'oscoin',\n",
       " 'varia',\n",
       " 'internalcomms',\n",
       " 'acierto',\n",
       " 'gasfee',\n",
       " 'nountown',\n",
       " 'virality',\n",
       " 'dirimidas',\n",
       " 'mailinglist',\n",
       " 'borget',\n",
       " 'domenic',\n",
       " 'cvxethcrv',\n",
       " 'blockchaincapital',\n",
       " 'evk',\n",
       " 'blocktower',\n",
       " 'gotm',\n",
       " 'punkscomic',\n",
       " 'mfw',\n",
       " 'eqfee',\n",
       " 'swellnetwork',\n",
       " 'yors',\n",
       " 'rihanna',\n",
       " 'importante',\n",
       " 'jaap',\n",
       " 'thurs',\n",
       " 'maci',\n",
       " 'mcap',\n",
       " 'lpxs',\n",
       " 'wagticorp',\n",
       " 'microincentives',\n",
       " 'lseg',\n",
       " 'jorpounder',\n",
       " 'moonpay',\n",
       " 'pspbp',\n",
       " 'vl',\n",
       " 'emo',\n",
       " 'fzagc',\n",
       " 'bodysuit',\n",
       " 'nailwal',\n",
       " 'remaning',\n",
       " 'dairbnb',\n",
       " 'respec',\n",
       " 'cusips',\n",
       " 'texto',\n",
       " 'portifolio',\n",
       " 'nodesguru',\n",
       " 'foxallocations',\n",
       " 'pcc',\n",
       " 'esque',\n",
       " 'aalpha',\n",
       " 'jambo',\n",
       " 'vorstand',\n",
       " 'tdic',\n",
       " 'homomorphic',\n",
       " 'lzweth',\n",
       " 'xfox',\n",
       " 'glo',\n",
       " 'pagerduty',\n",
       " 'premptive',\n",
       " 'chistyakov',\n",
       " 'avatarshape',\n",
       " 'mostpowerful',\n",
       " 'vincent',\n",
       " 'tokenlock',\n",
       " 'duppy',\n",
       " 'pami',\n",
       " 'greentatic',\n",
       " 'mohamed',\n",
       " 'composablestablepoolfactory',\n",
       " 'combate',\n",
       " 'listpm',\n",
       " 'giorgia',\n",
       " 'bitclout',\n",
       " 'cllothing',\n",
       " 'teravision',\n",
       " 'memeish',\n",
       " 'ephermeral',\n",
       " 'instituto',\n",
       " 'foley',\n",
       " 'gsn',\n",
       " 'senacomiyata',\n",
       " 'bmw',\n",
       " 'mcduck',\n",
       " 'rsks',\n",
       " 'kanpeki',\n",
       " 'lamboshi',\n",
       " 'hardhat',\n",
       " 'cielocapalini',\n",
       " 'kingmizomadeit',\n",
       " 'spokepools',\n",
       " 'recking',\n",
       " 'reprogram',\n",
       " 'edhec',\n",
       " 'antisocialite',\n",
       " 'joppe',\n",
       " 'bubbleworlds',\n",
       " 'fohmo',\n",
       " 'timelapse',\n",
       " 'vlah',\n",
       " 'telefon',\n",
       " 'notholy',\n",
       " 'greenlights',\n",
       " 'jorpounders',\n",
       " 'paxos',\n",
       " 'gaslimit',\n",
       " 'clment',\n",
       " 'grafana',\n",
       " 'ideapad',\n",
       " 'genesimmons',\n",
       " 'feeliquidation',\n",
       " 'chastain',\n",
       " 'ewf',\n",
       " 'fakegotchi',\n",
       " 'cme',\n",
       " 'paraboostscore',\n",
       " 'sinikiwe',\n",
       " 'revelointel',\n",
       " 'freeharry',\n",
       " 'bosa',\n",
       " 'buidlweek',\n",
       " 'leveragebuffer',\n",
       " 'whoa',\n",
       " 'decentrifi',\n",
       " 'chiptune',\n",
       " 'kia',\n",
       " 'nauzystan',\n",
       " 'liquidationpremium',\n",
       " 'aval',\n",
       " 'subquery',\n",
       " 'mowandmigrate',\n",
       " 'ufs',\n",
       " 'milad',\n",
       " 'headquaters',\n",
       " 'clemente',\n",
       " 'neurol',\n",
       " 'cryopreserved',\n",
       " 'stickier',\n",
       " 'awsome',\n",
       " 'especificado',\n",
       " 'reupload',\n",
       " 'ousdcrv',\n",
       " 'unicornpower',\n",
       " 'heredia',\n",
       " 'vnmrtz',\n",
       " 'afterwise',\n",
       " 'todo',\n",
       " 'katang',\n",
       " 'progeroid',\n",
       " 'overcollateralized',\n",
       " 'customgateway',\n",
       " 'arla',\n",
       " 'tokenratenotifier',\n",
       " 'bushoumono',\n",
       " 'mentallic',\n",
       " 'gauntletfeesetter',\n",
       " 'caregiving',\n",
       " 'nfteither',\n",
       " 'kadmil',\n",
       " 'veing',\n",
       " 'inscrito',\n",
       " 'kavascan',\n",
       " 'gamejams',\n",
       " 'twodams',\n",
       " 'redd',\n",
       " 'deployement',\n",
       " 'johngalt',\n",
       " 'mathutils',\n",
       " 'henrique',\n",
       " 'unconference',\n",
       " 'serena',\n",
       " 'mcfly',\n",
       " 'gleb',\n",
       " 'kzg',\n",
       " 'pawnfi',\n",
       " 'dipn',\n",
       " 'firb',\n",
       " 'boosterowner',\n",
       " 'discussionstypically',\n",
       " 'eithergot',\n",
       " 'nycolas',\n",
       " 'pradhan',\n",
       " 'amantay',\n",
       " 'paramater',\n",
       " 'bioactive',\n",
       " 'quantummiami',\n",
       " 'dsec',\n",
       " 'smartcon',\n",
       " 'rocketscientists',\n",
       " 'georg',\n",
       " 'dype',\n",
       " 'pendingltv',\n",
       " 'uvph',\n",
       " 'kenric',\n",
       " 'kanu',\n",
       " 'szns',\n",
       " 'baseflows',\n",
       " 'lmdbx',\n",
       " 'allowlist',\n",
       " 'jhyc',\n",
       " 'ayan',\n",
       " 'squarechainlabs',\n",
       " 'fbf',\n",
       " 'tweeks',\n",
       " 'zhao',\n",
       " 'reboarding',\n",
       " 'arestin',\n",
       " 'caveatbefore',\n",
       " 'relsts',\n",
       " 'dsa',\n",
       " 'metalabel',\n",
       " 'epxlorer',\n",
       " 'aqualand',\n",
       " 'alexandra',\n",
       " 'gimnez',\n",
       " 'motherboard',\n",
       " 'quantumspace',\n",
       " 'mulitchain',\n",
       " 'retracement',\n",
       " 'reconfigurefundingcyclesof',\n",
       " 'oficial',\n",
       " 'bridon',\n",
       " 'cie',\n",
       " 'aveqi',\n",
       " 'honn',\n",
       " 'runeverse',\n",
       " 'ishan',\n",
       " 'capt',\n",
       " 'erofeev',\n",
       " 'langauge',\n",
       " 'trackrecord',\n",
       " 'gandalf',\n",
       " 'flexi',\n",
       " 'gusdcrv',\n",
       " 'profmccarthy',\n",
       " 'imseih',\n",
       " 'develper',\n",
       " 'cvxcrvcrv',\n",
       " 'rdnaxela',\n",
       " 'zkrollups',\n",
       " 'codalabs',\n",
       " 'carrera',\n",
       " 'jessican',\n",
       " 'allowtokens',\n",
       " 'denylist',\n",
       " 'collateralise',\n",
       " 'sneha',\n",
       " 'barnbridges',\n",
       " 'helpimstreaming',\n",
       " 'feecollector',\n",
       " 'saranya',\n",
       " 'desencadenar',\n",
       " 'tolg',\n",
       " 'rakeback',\n",
       " 'troverse',\n",
       " 'canonicon',\n",
       " 'chiado',\n",
       " 'danya',\n",
       " 'stakinggovernance',\n",
       " 'reinitiated',\n",
       " 'tof',\n",
       " 'geland',\n",
       " 'subnets',\n",
       " 'pillon',\n",
       " 'polat',\n",
       " 'minimap',\n",
       " 'vaultfactory',\n",
       " 'biohybrid',\n",
       " 'yotphron',\n",
       " 'ethstore',\n",
       " 'donsnacks',\n",
       " 'ahmed',\n",
       " 'evt',\n",
       " 'prestaking',\n",
       " 'hashrate',\n",
       " 'stablecons',\n",
       " 'overbought',\n",
       " 'coderdan',\n",
       " 'nounsesports',\n",
       " 'hyperfocused',\n",
       " 'brkfstsndwchs',\n",
       " 'litenodes',\n",
       " 'hashi',\n",
       " 'masterized',\n",
       " 'cadogan',\n",
       " 'emergencia',\n",
       " 'carrington',\n",
       " 'airdro',\n",
       " 'arshields',\n",
       " 'executeable',\n",
       " 'rocklogic',\n",
       " 'srbw',\n",
       " 'parterning',\n",
       " 'linkkeep',\n",
       " 'markker',\n",
       " 'nonfungibility',\n",
       " 'strikethroughed',\n",
       " 'crema',\n",
       " 'riguetti',\n",
       " 'fabricant',\n",
       " 'elegidos',\n",
       " 'scrib',\n",
       " 'anatech',\n",
       " 'cbbe',\n",
       " 'holderness',\n",
       " 'extentions',\n",
       " 'fixedpricepsm',\n",
       " 'doyouknowwhereihavebean',\n",
       " 'noggle',\n",
       " 'niv',\n",
       " 'whadyou',\n",
       " 'omnipool',\n",
       " 'migratoraddress',\n",
       " 'timeboxed',\n",
       " 'kristoffer',\n",
       " 'bariatric',\n",
       " 'babypie',\n",
       " 'variablerateslope',\n",
       " 'deployers',\n",
       " 'sharpeneing',\n",
       " 'lootbox',\n",
       " 'maricela',\n",
       " 'essencia',\n",
       " 'cjthecryptokid',\n",
       " 'personalismos',\n",
       " 'lineas',\n",
       " 'jimpson',\n",
       " 'yubihsm',\n",
       " 'wsc',\n",
       " 'jonezin',\n",
       " 'evento',\n",
       " 'alsip',\n",
       " 'sohm',\n",
       " 'phuktep',\n",
       " 'kool',\n",
       " 'svestg',\n",
       " 'redelegation',\n",
       " 'lickd',\n",
       " 'ohanian',\n",
       " 'pinehurst',\n",
       " 'ozzz',\n",
       " 'coinbases',\n",
       " 'dota',\n",
       " 'mathew',\n",
       " 'belive',\n",
       " 'vimo',\n",
       " 'defisafety',\n",
       " 'maxyearlyratiogrowthpercent',\n",
       " 'killswitch',\n",
       " 'reccuring',\n",
       " 'dcltourguide',\n",
       " 'sujestion',\n",
       " 'circom',\n",
       " 'wido',\n",
       " 'ccmoons',\n",
       " 'chronosworlds',\n",
       " 'yambo',\n",
       " 'trgwlh',\n",
       " 'alextnetto',\n",
       " 'standalone',\n",
       " 'itba',\n",
       " 'metatrekkers',\n",
       " 'enmienda',\n",
       " 'cybergab',\n",
       " 'complexo',\n",
       " 'backlot',\n",
       " 'telx',\n",
       " 'grande',\n",
       " 'akr',\n",
       " 'appriciate',\n",
       " 'boosterownerlite',\n",
       " 'olimpio',\n",
       " 'cryptooracle',\n",
       " 'omics',\n",
       " 'sunambre',\n",
       " 'nadtochiy',\n",
       " 'programmable',\n",
       " 'solidityfinance',\n",
       " 'oplah',\n",
       " 'cryptolondoner',\n",
       " 'remeber',\n",
       " 'genart',\n",
       " 'eshop',\n",
       " 'groupto',\n",
       " 'cxdh',\n",
       " 'initiativesthe',\n",
       " 'isankar',\n",
       " 'tilty',\n",
       " 'thecryptosolider',\n",
       " 'manipulatable',\n",
       " 'dondochaka',\n",
       " 'stakehouse',\n",
       " 'openzeppelins',\n",
       " 'boombox',\n",
       " 'sharkdude',\n",
       " 'frenliest',\n",
       " 'dennett',\n",
       " 'conceptualist',\n",
       " 'neodyme',\n",
       " 'decentroland',\n",
       " 'cryptohun',\n",
       " 'awesomeness',\n",
       " 'staticusdplus',\n",
       " 'hurst',\n",
       " 'coinz',\n",
       " 'vestg',\n",
       " 'arpan',\n",
       " 'nisenson',\n",
       " 'bitrue',\n",
       " 'partneships',\n",
       " 'brrrrrrrr',\n",
       " 'visting',\n",
       " 'repricing',\n",
       " 'solhint',\n",
       " 'holdable',\n",
       " 'leanly',\n",
       " 'felipether',\n",
       " 'liang',\n",
       " 'theobtl',\n",
       " 'adcell',\n",
       " 'boostrapping',\n",
       " 'paradigma',\n",
       " 'longlpfund',\n",
       " 'lliquid',\n",
       " 'sony',\n",
       " 'bux',\n",
       " 'summonget',\n",
       " 'webclient',\n",
       " 'cabine',\n",
       " 'remasters',\n",
       " 'untransparent',\n",
       " 'cookiethief',\n",
       " 'mau',\n",
       " 'comerci',\n",
       " 'ayotemi',\n",
       " 'datachannel',\n",
       " 'ottersec',\n",
       " 'competion',\n",
       " 'aeyon',\n",
       " 'jhoico',\n",
       " 'mattgene',\n",
       " 'fontaine',\n",
       " 'runtimes',\n",
       " 'homersimps',\n",
       " 'skillsets',\n",
       " 'lga',\n",
       " 'uid',\n",
       " 'asa',\n",
       " 'chainsecurity',\n",
       " 'setor',\n",
       " 'teban',\n",
       " 'wusm',\n",
       " 'jewfucker',\n",
       " 'eof',\n",
       " 'yvcurve',\n",
       " 'garg',\n",
       " 'walletaddress',\n",
       " 'baselimit',\n",
       " 'ruzic',\n",
       " 'shuriken',\n",
       " 'chainids',\n",
       " 'communityin',\n",
       " 'councilor',\n",
       " 'bullape',\n",
       " 'kryptosphere',\n",
       " 'metaduckies',\n",
       " 'nounsclub',\n",
       " 'untilised',\n",
       " 'carsten',\n",
       " 'overanalyser',\n",
       " 'tangpoko',\n",
       " 'unagi',\n",
       " 'hypoxic',\n",
       " 'nuevas',\n",
       " 'essncia',\n",
       " 'blockbook',\n",
       " 'deathrattle',\n",
       " 'raregotchi',\n",
       " 'cryptocandy',\n",
       " 'boesin',\n",
       " 'kupay',\n",
       " 'versadia',\n",
       " 'unbribed',\n",
       " 'juliette',\n",
       " 'ordengg',\n",
       " 'quize',\n",
       " 'knoshua',\n",
       " 'vependle',\n",
       " 'niggeroni',\n",
       " 'nordicmike',\n",
       " 'dabplay',\n",
       " 'baixa',\n",
       " 'roatn',\n",
       " 'alvi',\n",
       " 'kickstarted',\n",
       " 'wdclapp',\n",
       " 'centralizatio',\n",
       " 'enthousiasts',\n",
       " 'shoutcaster',\n",
       " 'childporn',\n",
       " ...}"
      ]
     },
     "execution_count": 470,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "word_counts = Counter((\n",
    "    item \n",
    "    for sub in (\n",
    "        str(x).split() for x in df[\"body_nlp\"]\n",
    "    ) \n",
    "    for item in sub\n",
    "))\n",
    "len(word_counts)\n",
    "\n",
    "single = set([\n",
    "    word \n",
    "    for word, count in word_counts.items() \n",
    "    if  count <= 100 and not is_word_in_dictionary(word)\n",
    "])\n",
    "print(len(single))\n",
    "single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "func = skip_words(single)\n",
    "df[\"body_nlp\"] = func(df[\"body_nlp\"].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove remain crypto terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = Counter((\n",
    "    item \n",
    "    for sub in (\n",
    "        str(x).split() for x in df[\"body_nlp\"]\n",
    "    ) \n",
    "    for item in sub\n",
    "))\n",
    "len(word_counts)\n",
    "\n",
    "temp = pd.DataFrame({\"words\": word_counts.keys()})\n",
    "temp[\"count\"] = temp[\"words\"].apply(lambda x: word_counts[x])\n",
    "temp[\"dict\"] = temp[\"words\"].apply(lambda x: is_word_in_dictionary(x))\n",
    "temp[temp[\"dict\"]==False].sort_values(by=\"count\").to_csv(\"word_counts.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 480,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = [x for x in word_counts.keys() if \n",
    "    any(v in x for v in (\n",
    "        # \"kyc\", \"llama\", \"coinbase\",  \"codebase\", \"dune\", \"coingecko\", \"metamask\", \"certik\", \"okx\", \"defillama\"\n",
    "        #  \"unity\", \"opensea\", \"etherscan\", \n",
    "        \"aave\", \"cake\", \"decentraland\", \"arfc\", \"kalmy\",\n",
    "        \"bayc\", \"zk\", \"beethoven\", \"tmdc\", \"immunefi\", \"karpatkey\", \"alchem\",\n",
    "        \"fest\", \"vefxs\", \"osnap\", \"pixelcraft\", \"botb\", \"filipv\",  \"mayc\",\n",
    "        \"bancor\", \"poap\", \"stargate\", \"apechain\", \"karpatkey\", \"arrakis\",\n",
    "        \"gotchi\", \"openzeppelin\", \"agd\", \"pvfd\", \"merkl\", \"apecoins\",\n",
    "        \"certik\", \"ethdenver\", \"indexcoop\", \"dopewars\", \"jbp\", \"gamefi\",\n",
    "        \"halborn\", \"bgd\", \"baller\", \"wintermute\", \"dsr\", \"stax\", \"mario\", \"jpegd\", \"quo\", \"fip\",\n",
    "        \"zora\", \"euroc\", \"kh\",\n",
    "\n",
    "        # \"velodrome\", \"xdai\", \"jarvis\", \"circle\", \"ichi\", \"polkadot\", \"galxe\", \"aip\",\n",
    "        #  \"parcel\", \"gauntlet\",\"convex\", \"fantom\", \"tokenlogic\", \"gyroscope\", \"sushi\",\n",
    "        # \"beanstalk\",  \"fox\",\"chainlink\", \"thorchain\", \"cayman\", \"layerzero\", \"moon\", \n",
    "        \n",
    "        \"krause\", \"aura\", \"crv\", \"vlcvx\", \"ageur\", \"vebal\", \"vleqb\", \"vesdt\", \"veextra\", \"veangle\", \"vlmgp\" \"kpis\", \"matic\", \"aggold\",\n",
    "        )) and\n",
    "        not is_word_in_dictionary(x) ]\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dsr',\n",
       " 'arfc',\n",
       " 'bayc',\n",
       " 'mayc',\n",
       " 'certik',\n",
       " 'vesdt',\n",
       " 'krause',\n",
       " 'mario',\n",
       " 'stargate',\n",
       " 'bancor',\n",
       " 'vecake',\n",
       " 'poap',\n",
       " 'gotchiid',\n",
       " 'botb',\n",
       " 'fest',\n",
       " 'zora',\n",
       " 'programmatically',\n",
       " 'kh',\n",
       " 'pixelcraft',\n",
       " 'filipv',\n",
       " 'jbp',\n",
       " 'osnap',\n",
       " 'beethovenx',\n",
       " 'openzeppelin',\n",
       " 'ethdenver',\n",
       " 'wmatic',\n",
       " 'vecrv',\n",
       " 'ageur',\n",
       " 'bgd',\n",
       " 'vlaura',\n",
       " 'karpatkey',\n",
       " 'aavev',\n",
       " 'immunefi',\n",
       " 'stkaave',\n",
       " 'wintermute',\n",
       " 'merkl',\n",
       " 'halborn',\n",
       " 'agd',\n",
       " 'arrakis',\n",
       " 'gotchiverse',\n",
       " 'alchemica',\n",
       " 'quo',\n",
       " 'gamefi',\n",
       " 'vebal',\n",
       " 'vlcvx',\n",
       " 'vefxs',\n",
       " 'ballers',\n",
       " 'euroc',\n",
       " 'indexcoop',\n",
       " 'stax',\n",
       " 'vleqb',\n",
       " 'veangle',\n",
       " 'decentralands',\n",
       " 'veextra',\n",
       " 'fip',\n",
       " 'tmdc',\n",
       " 'pvfd',\n",
       " 'jpegd',\n",
       " 'aggold',\n",
       " 'apecoins',\n",
       " 'apechain',\n",
       " 'dopewars',\n",
       " 'kalmy']"
      ]
     },
     "execution_count": 481,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "func = skip_words(words)\n",
    "df[\"body_nlp\"] = func(df[\"body_nlp\"].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization: extract original word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [],
   "source": [
    "func = lambda x: ' '.join(\n",
    "       (singularize(xxx) for xxx in (lemma(xx) for xx in x.split()))\n",
    "    ).strip()\n",
    "func = np.vectorize(func)\n",
    "df[\"body_nlp\"] = func(df[\"body_nlp\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[(df[\"body_nlp\"]==\"nan\") | (df[\"body_nlp\"]==\"undefine\")  , \"body_nlp\"] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index-coop.eth            91\n",
      "conic-dao.eth             61\n",
      "cvx.eth                   55\n",
      "leagueoflils.eth          54\n",
      "gauges.aurafinance.eth    40\n",
      "muuu.eth                  28\n",
      "aurafinance.eth           15\n",
      "thelanddaonft.eth          1\n",
      "Name: space_id, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>space_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>351: Compensation for Anniversary Art Inclusio...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>leagueoflils.eth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>352: Buy an Alien CryptoPunk using stETH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>leagueoflils.eth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>354: Nouns DAO V3 upgrade, including Nouns For...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>leagueoflils.eth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>355: Nounify Ethereum Singapore with Public Goods</td>\n",
       "      <td>NaN</td>\n",
       "      <td>leagueoflils.eth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>369: Commercialization of the \"Noggle Changer\"!-</td>\n",
       "      <td>NaN</td>\n",
       "      <td>leagueoflils.eth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19836</th>\n",
       "      <td>USDT LAV for week of 20th Apr 2023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>conic-dao.eth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19837</th>\n",
       "      <td>USDT LAV for week of 25th Apr 2024</td>\n",
       "      <td>NaN</td>\n",
       "      <td>conic-dao.eth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19838</th>\n",
       "      <td>USDT LAV for week of 29th June 2023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>conic-dao.eth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19839</th>\n",
       "      <td>USDT LAV for week of 4th May 2023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>conic-dao.eth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19840</th>\n",
       "      <td>USDT LAV for week of 6th June 2024</td>\n",
       "      <td>NaN</td>\n",
       "      <td>conic-dao.eth</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>345 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title body  \\\n",
       "387    351: Compensation for Anniversary Art Inclusio...  NaN   \n",
       "388             352: Buy an Alien CryptoPunk using stETH  NaN   \n",
       "391    354: Nouns DAO V3 upgrade, including Nouns For...  NaN   \n",
       "393    355: Nounify Ethereum Singapore with Public Goods  NaN   \n",
       "395     369: Commercialization of the \"Noggle Changer\"!-  NaN   \n",
       "...                                                  ...  ...   \n",
       "19836                 USDT LAV for week of 20th Apr 2023  NaN   \n",
       "19837                 USDT LAV for week of 25th Apr 2024  NaN   \n",
       "19838                USDT LAV for week of 29th June 2023  NaN   \n",
       "19839                  USDT LAV for week of 4th May 2023  NaN   \n",
       "19840                 USDT LAV for week of 6th June 2024  NaN   \n",
       "\n",
       "               space_id  \n",
       "387    leagueoflils.eth  \n",
       "388    leagueoflils.eth  \n",
       "391    leagueoflils.eth  \n",
       "393    leagueoflils.eth  \n",
       "395    leagueoflils.eth  \n",
       "...                 ...  \n",
       "19836     conic-dao.eth  \n",
       "19837     conic-dao.eth  \n",
       "19838     conic-dao.eth  \n",
       "19839     conic-dao.eth  \n",
       "19840     conic-dao.eth  \n",
       "\n",
       "[345 rows x 3 columns]"
      ]
     },
     "execution_count": 495,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.replace('', None)\n",
    "print(df[df[\"body_nlp\"].isna()][\"space_id\"].value_counts())\n",
    "df[df[\"body_nlp\"].isna()][[\"title\", \"body\", \"space_id\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result of Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "body_nlp                     \n",
       "proposal vote newest proposal    12\n",
       "background reason                12\n",
       "vote follow proposal              6\n",
       "allocate vote proposal            4\n",
       "holder vote lock product          4\n",
       "holder vote k lock product        4\n",
       "prolong incentive have day        2\n",
       "proposal vote latest proposal     2\n",
       "proposal vote proposal            2\n",
       "distribute utc price              2\n",
       "amplification quorum              2\n",
       "treasury info vote vote           1\n",
       "read com                          1\n",
       "vote support sinful committee     1\n",
       "vote support committee            1\n",
       "propose decrease staker           1\n",
       "propose multisig repay pr         1\n",
       "vote proposal                     1\n",
       "vote committee                    1\n",
       "quorum acquisition                1\n",
       "return recoverable take forum     1\n",
       "tldr noun proposal                1\n",
       "see context option grant          1\n",
       "see forum                         1\n",
       "start saturday right week         1\n",
       "state leave do                    1\n",
       "user                              1\n",
       "syrup apr liquidity increase      1\n",
       "talk invalid question option      1\n",
       "tldr addres learn                 1\n",
       "proposal vote follow proposal     1\n",
       "other participate                 1\n",
       "proceed additional proposal       1\n",
       "please help earn                  1\n",
       "b c                               1\n",
       "b c invalid question option       1\n",
       "ban name                          1\n",
       "bearish                           1\n",
       "billion year somethe useful       1\n",
       "boost discussion                  1\n",
       "casino roulette even currency     1\n",
       "champion f curiou think           1\n",
       "community earn would burn         1\n",
       "con que se ha stake que da        1\n",
       "detail please read                1\n",
       "dont grateful thank               1\n",
       "forum ideation                    1\n",
       "gauge follow network quorum       1\n",
       "gauge follow quorum               1\n",
       "get scam help metamask            1\n",
       "go want holder pair pancake       1\n",
       "great join ur successively        1\n",
       "ifo launch session                1\n",
       "incentive duration day            1\n",
       "invalid question option           1\n",
       "official complete task list       1\n",
       "allocate vote proposal vote       1\n",
       "would see used st vector          1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 500,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = df[~df[\"body_nlp\"].isna()].copy()\n",
    "temp[\"nlp_len\"] = temp[\"body_nlp\"].apply(len)\n",
    "temp[temp[\"nlp_len\"] < 30][[\"body_nlp\"]].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(149,) 1980\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "cakevote.eth                          379\n",
       "index-coop.eth                        343\n",
       "snapshot.dcl.eth                      262\n",
       "cvx.eth                               115\n",
       "unidexapp.eth                          42\n",
       "decentralgames.eth                     30\n",
       "rndrnetwork.eth                        25\n",
       "glmrapes.eth                           25\n",
       "futera.eth                             24\n",
       "abracadabrabymerlinthemagician.eth     23\n",
       "badgerdao.eth                          23\n",
       "orbapp.eth                             21\n",
       "derify.eth                             20\n",
       "prismafinance.eth                      19\n",
       "arbitrumfoundation.eth                 18\n",
       "gnosis.eth                             18\n",
       "timelessfi.eth                         18\n",
       "stakewise.eth                          18\n",
       "theheaddao.eth                         17\n",
       "gramdao.eth                            17\n",
       "morpho.eth                             16\n",
       "snxgov.eth                             16\n",
       "lido-snapshot.eth                      16\n",
       "dinero.xyz                             15\n",
       "conic-dao.eth                          15\n",
       "shellprotocol.eth                      15\n",
       "alchemixstakers.eth                    14\n",
       "tomoondao.eth                          14\n",
       "leagueoflils.eth                       13\n",
       "olympusdao.eth                         12\n",
       "vote.airswap.eth                       11\n",
       "Name: space_id, dtype: int64"
      ]
     },
     "execution_count": 497,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = temp[temp[\"nlp_len\"] < 100][\"space_id\"].value_counts()\n",
    "print(temp.shape, temp.sum())\n",
    "temp[temp>10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With Categorized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20405, 41)"
      ]
     },
     "execution_count": 501,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ = df[~(df[\"body_nlp\"].isna())].copy()\n",
    "df_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "body_nlp                                                  \n",
       "propose increase feed increase leverage favmy decision         1\n",
       "amount receive receive receive                                 1\n",
       "android invalid question option                                1\n",
       "avatar yeah invalid question option                            1\n",
       "begin skyrocket amaze meeting acceleration skyrocket           1\n",
       "                                                              ..\n",
       "official airdrop limit offer guarantee                         3\n",
       "eagerly anticipate distribution enthusiastic participation     3\n",
       "vote decide whether technical review network foundation        7\n",
       "official announcement airdrop distribute supply                8\n",
       "holder choose change week control want reward                 26\n",
       "Length: 76, dtype: int64"
      ]
     },
     "execution_count": 502,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_class = df_[df_[class_cols].notnull().any(axis=1)].reset_index(drop=True).sort_values(by=['body_nlp'])\n",
    "df_class[\"nlp_len\"] = df_class[\"body_nlp\"].apply(len)\n",
    "df_class[df_class[\"nlp_len\"] < 60][[\"body_nlp\"]].value_counts().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1160, 42), (19245, 41))"
      ]
     },
     "execution_count": 503,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_not = df_[df_[class_cols].isnull().all(axis=1)].reset_index(drop=True).sort_values(by=['body_nlp'])\n",
    "\n",
    "df_class.shape, df_not.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "# X = vectorizer.fit_transform(df_class[\"body_nlp\"])\n",
    "# cos_mat = cosine_similarity(X, X)\n",
    "# print(cos_mat.shape)\n",
    "\n",
    "\n",
    "# threshold = 0.8\n",
    "# selected_rows = []\n",
    "# visited = set()\n",
    "\n",
    "# for i in range(len(df_class)):\n",
    "#     if i not in visited:\n",
    "#         selected_rows.append(i)\n",
    "#         for j in range(i + 1, len(df_class)):\n",
    "#             if cos_mat[i][j] >= threshold:\n",
    "#                 visited.add(j)\n",
    "\n",
    "# selected_rows = np.unique(np.array(selected_rows))\n",
    "# df_class = df_class.loc[selected_rows]\n",
    "# df_class.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1160, 19245)\n"
     ]
    }
   ],
   "source": [
    "df_body = pd.concat([df_class[\"body_nlp\"], df_not[\"body_nlp\"]])\n",
    "\n",
    "X = vectorizer.fit_transform(df_body)\n",
    "\n",
    "df_class_body = X[:df_class.shape[0]]\n",
    "df_not_body = X[df_class.shape[0]:]\n",
    "\n",
    "cos_mat = cosine_similarity(df_class_body, df_not_body)\n",
    "print(cos_mat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"similarity\", \"id_c\", \"space_id_c\", \"title_c\", \"body_c\", \"link_c\", \"id_n\", \"space_id_n\", \"title_n\", \"body_n\", \"link_n\"]\n",
    "df_columns = [\"id\", \"space_id\", \"title\", \"body\", \"link\"]\n",
    "\n",
    "def print_line(data):\n",
    "    print(\"[ID]:\", data[\"id\"])\n",
    "    print(\"[Space ID]:\", data[\"space_id\"])\n",
    "    print(\"[Title]:\", data[\"title\"])\n",
    "    print(data[\"body\"])\n",
    "\n",
    "def find_similar(threshold: int, threshold_upper: int=2, doPrint: bool = False):\n",
    "    global cos_mat, df_class, df_not, columns, class_cols, df_columns\n",
    "\n",
    "    temp = {k: None for k in columns + class_cols}\n",
    "    result = []\n",
    "    over = (cos_mat >= threshold) & (threshold_upper > cos_mat)\n",
    "\n",
    "    for i in (i for i, v in enumerate(over.sum(axis=1)) if v > 0):\n",
    "        y = df_class.iloc[i]\n",
    "        if doPrint:\n",
    "            print(\"-------------------------------------\")\n",
    "            print(\"-------------------------------------\")\n",
    "            print(\"[Category]:\", y[\"Class A\"], y[\"Class B\"], y[\"Class C\"], sep=\"\\t\")\n",
    "            print_line(y)\n",
    "            print(\"-------------------------------------\")\n",
    "        for v in df_columns:\n",
    "            temp[v+\"_c\"] = y[v]\n",
    "        for v in class_cols:\n",
    "            temp[v] = y[v]\n",
    "\n",
    "        \n",
    "        for j in (idx for idx, v in enumerate(over[i]) if v): \n",
    "            x = df_not.iloc[j]\n",
    "            if doPrint:\n",
    "                print(f\"{cos_mat[i, j]}\")\n",
    "                print_line(x)\n",
    "                print(\"======================\\n\\n\")\n",
    "            temp[\"similarity\"] = cos_mat[i, j]\n",
    "            for v in df_columns:\n",
    "                temp[v+\"_n\"] = x[v]\n",
    "            result.append(temp.copy())\n",
    "    return pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79, 14)\n",
      "(109, 14)\n",
      "(163, 14)\n",
      "(220, 14)\n"
     ]
    }
   ],
   "source": [
    "panel = [2, 1, 0.9, 0.8, 0.75] # 0.6은 별로\n",
    "df_panel = {}\n",
    "for i in range(len(panel)-1):\n",
    "    result = find_similar(threshold=panel[i+1], threshold_upper=panel[i])\n",
    "    df_panel[f\"over{panel[i+1]}\"] = result\n",
    "    print(result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_c</th>\n",
       "      <th>body_c</th>\n",
       "      <th>title_n</th>\n",
       "      <th>body_n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Season 4 Governance Council Election</td>\n",
       "      <td>Abstract\\nAs the CUIP-004 - Governance Council...</td>\n",
       "      <td>Season 3 Governance Council Election</td>\n",
       "      <td>Abstract\\nAs the CUIP-004 - Governance Council...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Season 2 Governance Council Election</td>\n",
       "      <td>Abstract\\nAs the CUIP-004 - Governance Council...</td>\n",
       "      <td>Season 3 Governance Council Election</td>\n",
       "      <td>Abstract\\nAs the CUIP-004 - Governance Council...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fantastic news for all Angle users!</td>\n",
       "      <td>Calling all Angle users! We're delighted to sh...</td>\n",
       "      <td>Absolutely thrilling news for all Lido users!</td>\n",
       "      <td>Calling all Lido users! We're delighted to sha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fantastic news for all rhinofi users!</td>\n",
       "      <td>Calling all rhinofi users! We're delighted to ...</td>\n",
       "      <td>Absolutely thrilling news for all Lido users!</td>\n",
       "      <td>Calling all Lido users! We're delighted to sha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fantastic news for all Angle Protocol users!</td>\n",
       "      <td>Calling all Angle Protocol users! We're deligh...</td>\n",
       "      <td>Absolutely thrilling news for all Lido users!</td>\n",
       "      <td>Calling all Lido users! We're delighted to sha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        title_c  \\\n",
       "0          Season 4 Governance Council Election   \n",
       "1          Season 2 Governance Council Election   \n",
       "2           Fantastic news for all Angle users!   \n",
       "3         Fantastic news for all rhinofi users!   \n",
       "4  Fantastic news for all Angle Protocol users!   \n",
       "\n",
       "                                              body_c  \\\n",
       "0  Abstract\\nAs the CUIP-004 - Governance Council...   \n",
       "1  Abstract\\nAs the CUIP-004 - Governance Council...   \n",
       "2  Calling all Angle users! We're delighted to sh...   \n",
       "3  Calling all rhinofi users! We're delighted to ...   \n",
       "4  Calling all Angle Protocol users! We're deligh...   \n",
       "\n",
       "                                         title_n  \\\n",
       "0           Season 3 Governance Council Election   \n",
       "1           Season 3 Governance Council Election   \n",
       "2  Absolutely thrilling news for all Lido users!   \n",
       "3  Absolutely thrilling news for all Lido users!   \n",
       "4  Absolutely thrilling news for all Lido users!   \n",
       "\n",
       "                                              body_n  \n",
       "0  Abstract\\nAs the CUIP-004 - Governance Council...  \n",
       "1  Abstract\\nAs the CUIP-004 - Governance Council...  \n",
       "2  Calling all Lido users! We're delighted to sha...  \n",
       "3  Calling all Lido users! We're delighted to sha...  \n",
       "4  Calling all Lido users! We're delighted to sha...  "
      ]
     },
     "execution_count": 508,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_panel[\"over1\"][[\"title_c\", \"body_c\", \"title_n\", \"body_n\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = 'similar_categorized_filtered.xlsx'\n",
    "\n",
    "with pd.ExcelWriter(output_file, engine='xlsxwriter') as writer:\n",
    "    for k in df_panel.keys():\n",
    "        df_panel[k].to_excel(writer, sheet_name=k, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over1 Empty DataFrame\n",
      "Columns: [id_n, Class A, Class B]\n",
      "Index: []\n",
      "(19, 3)\n",
      "over0.9 Empty DataFrame\n",
      "Columns: [id_n, Class A, Class B]\n",
      "Index: []\n",
      "(31, 3)\n",
      "over0.8 Empty DataFrame\n",
      "Columns: [id_n, Class A, Class B]\n",
      "Index: []\n",
      "(57, 3)\n",
      "over0.75 Empty DataFrame\n",
      "Columns: [id_n, Class A, Class B]\n",
      "Index: []\n",
      "(58, 3)\n"
     ]
    }
   ],
   "source": [
    "df = df.sort_values(by=[\"id\"])\n",
    "\n",
    "pool = set()\n",
    "for k in df_panel.keys():\n",
    "    if df_panel[k].empty:\n",
    "        continue\n",
    "    df_panel[k] = df_panel[k].sort_values(by=[\"id_n\"])\n",
    "\n",
    "    groups = df_panel[k].groupby([\"id_n\", \"Class A\", \"Class B\"], dropna=False)\n",
    "    t = groups.count().index.to_frame().reset_index(drop=True)\n",
    "    temp = t[\"id_n\"].value_counts()\n",
    "    temp_x = temp[temp>1].index\n",
    "    temp = temp[~(temp>1)].index\n",
    "    print(k,  t[t[\"id_n\"].isin(temp_x)])\n",
    "    temp = t[t[\"id_n\"].isin(temp) & ~t[\"id_n\"].isin(pool)]\n",
    "\n",
    "    if (df.loc[df[\"id\"].isin(temp[\"id_n\"])][\"id\"].values == temp[\"id_n\"].values).all():\n",
    "        df.loc[df[\"id\"].isin(temp[\"id_n\"]), [\"id\", \"Class A\", \"Class B\"]] = temp.values\n",
    "        print(temp.values.shape)\n",
    "        pool.update(set(temp.values.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1359, 41)"
      ]
     },
     "execution_count": 511,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[class_cols].notnull().any(axis=1)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(\"proposals_preprocess_0812_nlp.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Without Categorized Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -q networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_not = df_not[~df_not[\"id\"].isin(df_panel[\"over1\"][\"id_n\"])]\n",
    "df_not = df_not[~df_not[\"id\"].isin(df_panel[\"over0.9\"][\"id_n\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19195, 19195)"
      ]
     },
     "execution_count": 514,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(df_not[\"body_nlp\"])\n",
    "cos_mat = cosine_similarity(X, X)\n",
    "cos_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "def find_similar_cluster(threshold: int, threshold_upper:int=2):\n",
    "    G = nx.Graph()\n",
    "    for idx in range(len(df_not)):\n",
    "        G.add_node(df_not.iloc[idx][\"id\"])\n",
    "\n",
    "    over = (cos_mat >= threshold) & (threshold_upper > cos_mat) \n",
    "    for i in range(len(df_not)):\n",
    "        for j in range(i + 1, len(df_not)):\n",
    "            if over[i, j]:\n",
    "                G.add_edge(df_not.iloc[i][\"id\"], df_not.iloc[j][\"id\"], weight=cos_mat[i, j])\n",
    "\n",
    "    filtered_components = [\n",
    "        df_not[df_not[\"id\"].isin(list(c))][[\"id\", \"space_id\", \"title\", \"body\", \"link\", \"Class A\", \"Class B\", \"Class C\"]]\n",
    "        for c in list(nx.connected_components(G)) \n",
    "        if len(c) >= 2\n",
    "    ]\n",
    "    filtered_components = sorted(filtered_components, key=lambda x: len(x), reverse=True)\n",
    "\n",
    "    rows = 0\n",
    "    output_file = f'df_not_{threshold}_nlp.xlsx'\n",
    "    with pd.ExcelWriter(output_file, engine='xlsxwriter') as writer:\n",
    "        for i, component in enumerate(filtered_components):\n",
    "            component.to_excel(writer, sheet_name=str(i), index=False)\n",
    "            rows += component.shape[0]\n",
    "    print(f\"Number of rows with threshold {threshold}: {rows}\")\n",
    "    return filtered_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with threshold 1: 1915\n",
      "1 579\n",
      "Number of rows with threshold 0.9: 3397\n",
      "0.9 980\n",
      "Number of rows with threshold 0.8: 2686\n",
      "0.8 570\n"
     ]
    }
   ],
   "source": [
    "panel = [2, 1, 0.9, 0.8]\n",
    "for i in range(len(panel)-1):\n",
    "    filtered_components = find_similar_cluster(threshold=panel[i+1], threshold_upper=panel[i])\n",
    "    print(panel[i+1], len(filtered_components))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
